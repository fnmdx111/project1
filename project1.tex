\documentclass{article}

\usepackage{hyperref}
\usepackage{natbib}
\usepackage{float}


\title{W4112 Project 1}

\begin{document}
\section{Big Data Analysis and Relational Database Management Systems}

In the following section, we would like to look into big data analysis frameworks and how they impact on database management systems.

Because of the burst of technology development, people are more connected to the Internet than ever before, thus we're having PB's of data generated everyday. For example, people are doing more and more online shopping, which produces a whole lot more information for analysts to dig into more deeply. Traditional techniques seem inadequate for data sets of this kind of scale. 

The problem of this extends naturally to database management systems: people are nowadays creating, maintaining and querying bigger and bigger tables with more and more rows and columns. It also impairs the usability of traditional database management systems that relational database systems cannot categorize unstructured data like pictures, audios and videos, whereas they exist ubiquitously in today's data sets.\cite{bigs1} It seems obvious that we need new techniques for RDMSes, rather than just tolerate with the old ones, to cope with big data.

\subsection{MapReduce-based Frameworks}

It has become a major concern on how to process and analyze very large data sets, and people realized that there is a (or are many) common pattern(s) about how we can process very large data sets. One among many successful attempts is called \textbf{MapReduce}.

MapReduce is the solution given by Google to process large (over $1TB$) data sets parallelly. It concerns mainly two concepts: \textit{Map} and \textit{Reduce}. A Mapper (or mapping function) is an operation on a list, where each element of the list is applied to the same function (i.e. the mapping function). After this mapping procedure, a Reducer is used to collect all the results from the mapper and reduce all the results into a single result. In functional programming, if the mapping function is free of side-effects, the mapping procedure is spontaneously parallelable because each sub-procedure can be seen as isolated from each other, so that it suffices that the order of these sub-procedures doesn't matter. (The reduce procedure is, however, less parallelable in general.) By utilizing this nature and adding mechanisms for fault-tolerance and redundancy, MapReduce framework is able to make good use of distributed computing resources by distributing map tasks to multiple instances.\cite{wiki:mapreduce} From the perspective of programmers, the MapReduce model requires programmers to have little or no prior knowledge on distributed algorithms to create an application that could run in parallel across multiple nodes in the cluster.\cite{xie2010improving}

It may be a good fit between the MapReduce model and relational database management systems. When querying a very large table, the system could use the MapReduce model to distribute the query, i.e. let each instance do the query on a subset of the whole table, and finally merge the sub-results. This would, of course, require the data to be parallel accessible.

\subsubsection{Apache Hadoop}

The MapReduce paper has inspired lots of open-source implementations, of which the most famous one is Apache Hadoop. So it is beneficial that we look at Hadoop in detail. Hadoop consists of two core parts, namely the storage part and processing part.

\begin{description}
\item[HDFS] The storage part is called Hadoop Distributed Filesystem, or HDFS. It is a distributed, scalable and portable filesystem specifically designed for the Hadoop framework, but also is also applicable to other uses, of which include the HBase database, the Apache Mahout machine learning system and the Apache Hive Data Warehouse sytem.\cite{wiki:hadoop}

A typical instance of HDFS consists of one NameNode which keeps the directory tree of all the files in the filesystem and several DataNode that actually holds all the data. When a client wants to do certain actions to the HDFS instance, it is actually talking to the NameNode, for example, the NameNode responds with a list of relevant DataNode servers where the data resides to the client via TCP/IP sockets, if the client wish to read a certain file.\cite{hwiki:hadoop} Replication is done across the DataNodes to support failure-tolerance.

A major advantage with HDFS is that HDFS is aware of data location and is willing to use such knowledge to reduce the amount of traffic that goes through the network and unnecessarily moved data. The job tracker and task tracker can then use this information to better schedule how the data will be accessed. This is especially important since it is critical to cut down the cost of communication for a good MapReduce algorithm.\cite{ullman2012designing} It is, however, important to point out that HDFS is good for files that rarely got updated, rather than system requiring frequent concurrent. write-operations.\cite{wiki:hadoop}

\item[MapReduce Engine] The processing part that consists of one JobTracker and multiple TaskTrackers, is built on top of the storage part as stated above. User submits MapReduce jobs to JobTracker, where JobTracker distributes the jobs to the TaskTrackers that are alive and have sufficient slot to receive jobs. In a data-aware filesystem (for example, HDFS), the JobTracker is able to distribute the job to the node as close as possible to the node which actually holds the data.\cite{wiki:hadoop}

A TaskTracker can only receive new jobs if it has enough free slots. Active map or reduce task each takes up one free slot. New jobs are allocated to the TaskTracker that is closest to the DataNode with no considerations about the work load of the current node, which could lead to inefficiency of the whole Hadoop system. The JobTracker supports various scheduler algorithms to cope with different kinds of job patterns, for example, the fair scheduler developed by Facebook aims for fast response times for small jobs and QoS for production jobs.
\end{description}

The importance of the Apache Hadoop framework cannot be addressed enough. It enables data center owners to fully utilize whatever computational power they have to store and process more data than ever before within the same amount of time fault-tolerantly. And since it's an open-source framework, a lot of subsequent works are done, basing on Apache Hadoop. So it's suffice to say that Hadoop has now become the name of an ecosystem, where different projects targeting different tasks are built around Apache Hadoop. To name but a few, Apache Pig, Apache Hive, Apache HBase and so on.

\subsubsection{Apache Pig}

Although we now have an adequate enough framework to run our MapReduce applications, it is hard for programmer to write good MapReduce code. This is because\cite{mortar}
\begin{itemize}
\item The MapReduce model is unintuitive for programmers without a functional programming background.
\item Real-world applications can be too hard to model and manage in plain code.
\item Boilerplate code is ubiquitously needed to support common operations and custom data types.
\end{itemize}

That is to say a higher-level abstraction of MapReduce programming is needed. People realized this and created Apache Pig. Apache Pig is an important part of the Hadoop ecosystem. It is a platform for analyzing large data sets built on top of Apache Hadoop. Pig also contains a high-level language called Pig Latin that is expressive when writing data analysis programs and naturally amenable to automatic parallelization.

Programs coded in Pig is quite analogous to SQL at first glance, however, there exists several critical differences.

The major difference is that Pig Latin is procedural, while SQL is declarative. Classic procedural programming language includes C, Go and Fortran, whereas SQL and most functional languages are considered declaractive programming language. The key difference between these two paradigm is that users have to explicitly give a series of procedures for the language to call. The users of declarative languages, however, declare what to do before the language figures out how to do (classic application scenario of query optimizers of SQL). This programming paradigm that Pig Latin utilizes is very easy to use when the users want to do a series of computations with the data, where the output of each computation is the input of the next computation (also called pipeline paradigm\cite{wiki:pig}).

From the major difference discussed above, another two difference can be derived:
\begin{itemize}
\item In Pig, users are forced to program how the system will carry
  out the query, while in SQL, the query optimization is responsible
  for that. This difference makes sure that programmers are more
  flexible in controlling how the data stream flows in the program.
\item With some effort utilizing the properties of pipelines, Pig is
  able to store data, execute other code during a pipeline and run
  different pipeline code on different parts of the data
  stream. Whereas in SQL, none of the above is easy or possible to
  implement.
\end{itemize}

\subsubsection{Apache Hive}

Despite the simplicity of Apache Pig, programmers would still prefer something more akin to SQL for querying a database (not necessarily a RDBMS). There are several reasons to that:

\begin{itemize}
\item There is much less to learn if one can use SQL-like language to operate on the database.
\item Programmers can query a new database with the same old code. For example, people can use the same piece of code that does something through the JDBC interface with a database that's based on Apache Hadoop and enjoy the high performance brought by it, and immediately switch back to the original database for some other purposes.
\item Sometimes it is just easier to express the query using declarative code than procedural code. The relational model and constructs of SQL support data warehousing the best. Users often write ad-hoc queries in SQL more quickly than other kind of query languages\cite{pig-at-yahoo} (basically due to its declaractive nature, which is elaborated before).
\end{itemize}

Following these requirements from practice, Apache Hive was invented. According to Hive's homepage, Apache Hive is a data warehouse software system that aims at querying and managing large datasets across distributed storage with a SQL-like (although not compatible with the full SQL-92 standard) language called HiveQL. At a subsequent version, HiveQL was made compliant with full ACID functionality and provided indexes for faster queries.

The internal compiler will compile the HiveQL commands into MapReduce, Apache Tez or Spark jobs and submit them to the underlying Apache Hadoop instance.\cite{wiki:hive} That is to say, Apache Hive uses HDFS as its storage, which is proven inefficient for write operations. It also has only limited supports on transactions. But none of these reasons stops Apache Hive from being one of the best data warehouse infrastructures.

There exists lots of cloud services provide Apache Hadoop (called Hadoop as a Service Provider) including AWS, Microsoft Azure, Cloudera, etc. Most of them also supports Apache Pig and Apache Hive. Apart from cloud offerings, Apache Hadoop is the foundation of many other libraries or applications from Apache. It is also distributed as a part of large scale computation package by IBM, Intel and many others with proprietary modifications.\cite{top-10-hadoop}


\subsection{Google Dremel}

It is well-known that Hadoop and its related software systems provide less than ideal performance, which impairs the use of the Hadoop, Pig or Hive in time-intensive or real-time applications. Query executions can often take up several hours, which is nowhere near the performance modern DBMS can offer, i.e. it is only good for offline batch processing where latency is not of an issue.\cite{kalavri2013mapreduce} To cope with this limitation, Google created a scalable, interactive ad-hoc query system for read-only nested data that can access data in place.\cite{melnik2010dremel}

On a higher scale, the architecture of Dremel and that of Pig or Hive are quite alike. The major difference is that instead of translating input to MapReduce jobs as what Pig and Hive do, Dremel runs the jobs on a novel query execution engine. The engine executes the queries using aggregator trees, which generates result by aggregating replies from lower levels of the tree. The other reason that Dremel is faster is because of the columnar storage it uses, which is more efficient when striping a file into columns.\cite{melnik2010dremel}

Dremel is proprietary software widely used internally at Google. It also powers the BigQuery service hosted by Google.

\subsubsection{Apache Drill}

Apache Drill is an open-source framework inspired by Google's Dremel that targets at the same goal with Dremel. Supported by a plugin system, Apache Drill can query data from heterogeneous datastores including Hadoop, various NoSQL and cloud storages. Apache Drill is aware of the importance of SQL and supports ANSI SQL, ODBC and JDBC drivers, which enables the integration with Business Intelligence tools.

By supporting schema-free data model, Apache Drill is able to query in situ NoSQL and cloud datastores, in other words, users can execute queries without first knowing the structure of the data and still achieve high execution speed, this is extremely usefule when exploring raw data.\cite{intro-to-drill} Despite this, Apache Drill still is able to take advantage of schemas when possible.\cite{faq-drill} Analogous to HDFS, Apache Drill is datastore-aware, that is to say, the query optimizer knows the availability of each datastore and is able to recompile the query so that, for example, network traffic is reduced.\cite{wiki:drill}

\subsubsection{Shark}

Shark is another attempt at interactive (or fast) data querying and processing. Shark is open source and built on top of the Hive codebase, meaning that it is compatible with Apache Hive, but provides a major speedup. It is based on SQL just like Google Dremel and Apache Drill, however, instead of implementing new execution engines, Shark still relies on the classic MapReduce model but with a novel distributed memory abstraction to achieve query efficiency. By using this novel distributed memory abstraction called Resilient Distributed Datasets, Shark is able to do most of the analytic computations in memory, which is important because:

\begin{itemize}
\item studies show that workloads of SQL warehouse show strong temporal and spatial locality, that is to say, (1) when a piece of data is needed, it is very likely to be needed again; (2) when a piece of data is needed, the next piece of data is very likely to be needed
\item many complex analytic computations require scanning over the datastore for a number of times, where by putting data into memory, the time consumed by accessing the data is reduced by a large amount.\cite{xin2013shark}
\end{itemize}

Shark is now a part of Apache Spark and renamed to Spark SQL.\cite{shark-homepage}

\subsection{Dryad and Spark}

It is still perceived by the majority that parallel and distributed programming is hard for programmers even if we had a generic library at hand. While Apache Pig tackles at this problem by creating new programming language, people at Microsoft Research see the problem from a difference angle and created a new high-performance distributed execution engine for general-purpose programming tasks and application.\cite{isard2007dryad}

Dryad is inspired by the Shader language (along with the MapReduce framework and parallel databases) that is specifically tuned to accommodate special hardware architecture and memory access patterns in graphics cards. A graphics process unit (or GPU) has hundreds of cores, each core is able to do whatever computations assigned to it on its own. Because of this, the computations done on a graphics card is highly parallelized. This is especially good for the common graphics computational tasks, where there are many vertices to be shaded, and the algorithm for shading each vertex is the same. The natural architecture of the graphics cards makes it possible for them to concentrate on shading algorithms.

Dryad tries to abstract a general computational model from the three classic distributed model, and found that it would be best for the developers to, in the above analogy, control over what algorithm to run at each core (or vertex) and the executional topological order of each vertex. In this design, the mechanisms for concurrency control, resource allocation, scheduling, failure recovering, etc. can be hidden into the Dryad system runtime, and the programmers are only to decide the communication graph and the algorithms that run in vertices.\cite{isard2007dryad}

When developers program in the Dryad system, they are acutally programming a directed acyclic graph (DAG), which exhibits the dataflow of the application. Programmers are to program each vertex what to do with the data that flow into the vertex. By adding an edge in the DAG (even when the system is online), programmers are able to define how the data will flow across the whole application. In details, such edge is implemented as communication channels utilizing various methods, for example, TCP/IP sockets, restricted shared memory.\cite{wiki:dryad} The use of shared memory and temporary files other than TCP/IP sockets seems to be a good replacement for the potential bottleneck caused by networking latencies. Because Dryad restricts programmers to write purely sequential programs with no thread creation or locking\cite{isard2007dryad}, there won't be any race conditions, thus usage of shared memory and temporary files is parallel-safe.

We believe that Dryad is a more generalize distributed programming framework than the MapReduce model, because in MapReduce, developers are not able to control the mapped jobs in a finer granuality. The framework automatically hides them from developers.

Microsoft Research shutdown the Dryad project in late 2011 and turned to Apache Hadoop for its own Azure service.

\subsubsection{DryadLINQ}

Just like the reason people created Apache Pig, nobody actually directly programs on the Dryad system. It is difficult for developers to design correct DAGs and vertex programs to faithfully implement whatever algorithm they want. As a consequence, most of the users use DryadLINQ as the frontend of the Dryad system.\cite{youtube-dryad}

Language-Integrated Query (or LINQ) is a domain-specific language that enables programmers to query native data in the .NET language family using SQL-like statements. DryadLINQ is a system that takes advantage of this property of LINQ, compiles code written in LINQ to sequential program, and automatically parallelize the program wherever possible into a distributed execution plan (or to say a directed acyclic graph). After the above procedure, the compiled code is sent the Dryad system for execution.\cite{yu2008dryadlinq} Although it is evaluated that the performance of Dryad and DryadLINQ is excellent and scales near-linearly, it still is not appropriate for low-latency applications.

\subsection{Apache Spark}

Apache Spark was originally developed by the same group of researchers who developed Shark at UC Berkeley and aimed at low-latency distributed computing. It is a open-source general-purpose distributed computing framework competing and outperforming Hadoop.

The first idea of Spark is that programmers are provided with a key data structure called Resilient Distributed Dataset (or RDD for short) that maps to read-only a dataset potentially distributed across multiple nodes of the cluster as distributed shared memory. Iterative computations can be very efficient using RDDs so that applications using RDDs are marked as low-latency. This is extremely useful in implementing training algorithms for machine learning systems as the dataset of such training can be enormous. It is, in fact, the motivation of Spark to support such iterative algorithms.\cite{wiki:spark} The second idea is one of the same as Shark, i.e. to use as much memory as possible for all the computations that might happen. By utilizing memory resource, the IO efficient can be improved by several orders of magnitude.

Apache Spark consists of Spark Core, and other modules, each of which is specialized to a particular task, for instance, Spark SQL (previously Shark) enables programmers to operate on the datasets over multiple datastores with SQL, while Spark MLlib contains various machine learning algorithms like SVM and PCA.\cite{wiki:spark} The Spark Core is responsible for task dispatching, scheduling and IO, which are callable by programmers in the form of manipulations to RDDs. The operations programmed by developers takes RDDs and in turn produces new RDDs for next steps of computations to use. This is very much like what Dryad does with a directed acyclic graph, where each node takes data flowed in (RDDs as input), and produces new data (new RDDs as the computational results) the flows out via the edges.

Like Apache Hadoop, Apache Spark is considered a successful distributed computing framework and is widely supported by cloud service providers including Amazon, Microsoft, etc. Databricks, IBM and others provide offline commercial version of Apache Spark.

\bibliographystyle{plain}
\nocite{*}
\bibliography{bib}

\end{document}